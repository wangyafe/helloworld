1.（爬虫）利用仿站工具，批量的抓取页面到本地，适合页面类型多的网站
 1.1 需要在服务器端做好设置，允许正常的用户（浏览器UA）以及正常的搜索引擎蜘蛛（搜索蜘蛛UA,例如百度Baiduspider）访问网站的页面，禁止非法的UA，比如仿站工具前来爬取页面内容，所以只要配置好禁止爬取的UA即可
2.纯人工去复制页面，这样的方法适合于页面类型少，页面简单。只要在页面上做好相应的防护措施即可
 2.1 1.屏蔽鼠标右键 2.屏蔽Ctrl+s 保存页面 3.屏蔽Ctrl+u 查看页面的源代码 4.屏蔽F12 5.屏蔽Ctrl+shift+i 屏蔽调出控制台 和F12一样
3.当页面被整个盗取到本地的时候，本地打开一片空白，需要jquery（原理：检查一下当前页面域名）
4.JS代码可以再做一次加密来增加破解者的破解难度
5.网站被别人轻易复制？如何加密网站？解决方法如下： https://www.louishe.com/2018/02/06/doc-613.html
